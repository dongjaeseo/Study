{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "diabetes1.py",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNj7WYQ0qaddVIkMuUGPPg1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dongjaeseo/study/blob/main/diabetes1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9qKo8O7N3Cw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e83c381a-15ea-47b2-8021-7298c24b57cf"
      },
      "source": [
        "import numpy as np\r\n",
        "\r\n",
        "from sklearn.datasets import load_diabetes\r\n",
        "\r\n",
        "dataset = load_diabetes()\r\n",
        "x = dataset.data\r\n",
        "y = dataset.target\r\n",
        "\r\n",
        "\r\n",
        "from sklearn.model_selection import train_test_split as tts\r\n",
        "x_train,x_test,y_train,y_test = tts(x,y,train_size = 0.8, shuffle = True)\r\n",
        "\r\n",
        "from sklearn.model_selection import train_test_split as tts\r\n",
        "x_train,x_val,y_train,y_val = tts(x_train,y_train,train_size = 0.8, shuffle = True)\r\n",
        "\r\n",
        "from sklearn.preprocessing import StandardScaler\r\n",
        "scaler = StandardScaler()\r\n",
        "scaler.fit(x_train)\r\n",
        "x_train = scaler.transform(x_train)\r\n",
        "x_test = scaler.transform(x_test)\r\n",
        "x_val = scaler.transform(x_val)\r\n",
        "\r\n",
        "\r\n",
        "#2. model\r\n",
        "from tensorflow.keras.models import Model, Sequential\r\n",
        "from tensorflow.keras.layers import Dense, Input\r\n",
        "\r\n",
        "input = Input(shape = (10,))\r\n",
        "d = Dense(64, activation = 'relu')(input)\r\n",
        "d = Dense(64)(d)\r\n",
        "d = Dense(64)(d)\r\n",
        "d = Dense(64)(d)\r\n",
        "d = Dense(64)(d)\r\n",
        "d = Dense(64)(d)\r\n",
        "d = Dense(1)(d)\r\n",
        "\r\n",
        "model = Model(inputs = input, outputs = d)\r\n",
        "\r\n",
        "from tensorflow.keras.callbacks import EarlyStopping\r\n",
        "early_stopping = EarlyStopping(monitor = 'loss', patience = 10, mode = 'auto')\r\n",
        "\r\n",
        "#3. compile fit\r\n",
        "model.compile(loss = 'mse', optimizer = 'adam', metrics = 'mae')\r\n",
        "model.fit(x_train,y_train,epochs = 2000, validation_data = (x_val,y_val), batch_size = 8, callbacks = [early_stopping])\r\n",
        "\r\n",
        "#4. evaluation, predict\r\n",
        "loss, mae = model.evaluate(x_test,y_test, batch_size = 8)\r\n",
        "print('loss : ', loss)\r\n",
        "print('mae : ',mae)\r\n",
        "\r\n",
        "y_pred = model.predict(x_test)\r\n",
        "\r\n",
        "from sklearn.metrics import mean_squared_error, r2_score\r\n",
        "def rmse(a,b):\r\n",
        "  return np.sqrt(mean_squared_error(a,b))\r\n",
        "\r\n",
        "print('r2 : ', r2_score(y_pred,y_test)) \r\n",
        "\r\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2000\n",
            "36/36 [==============================] - 2s 13ms/step - loss: 21895.5653 - mae: 126.9813 - val_loss: 5750.3628 - val_mae: 60.9033\n",
            "Epoch 2/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 4614.1925 - mae: 53.0881 - val_loss: 4979.8369 - val_mae: 54.5567\n",
            "Epoch 3/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 4370.0407 - mae: 51.2938 - val_loss: 4259.1226 - val_mae: 51.2224\n",
            "Epoch 4/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 4496.7127 - mae: 53.5540 - val_loss: 4021.7061 - val_mae: 47.7658\n",
            "Epoch 5/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 3656.1291 - mae: 48.3953 - val_loss: 3794.9753 - val_mae: 46.4106\n",
            "Epoch 6/2000\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 3120.5097 - mae: 45.7809 - val_loss: 3778.0547 - val_mae: 46.0151\n",
            "Epoch 7/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 3134.0486 - mae: 46.7703 - val_loss: 3687.4451 - val_mae: 45.3894\n",
            "Epoch 8/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 3009.1850 - mae: 43.9852 - val_loss: 3593.0344 - val_mae: 45.0698\n",
            "Epoch 9/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 2866.5792 - mae: 43.8860 - val_loss: 3650.3538 - val_mae: 46.2763\n",
            "Epoch 10/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 2665.4939 - mae: 43.1871 - val_loss: 3594.5383 - val_mae: 44.5039\n",
            "Epoch 11/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 2532.1470 - mae: 41.7507 - val_loss: 3533.9001 - val_mae: 44.0191\n",
            "Epoch 12/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 2750.3039 - mae: 43.1205 - val_loss: 3524.6792 - val_mae: 44.1029\n",
            "Epoch 13/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 2531.0374 - mae: 40.4976 - val_loss: 3542.4612 - val_mae: 44.3165\n",
            "Epoch 14/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 2501.8443 - mae: 40.9528 - val_loss: 3842.1145 - val_mae: 46.9501\n",
            "Epoch 15/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 2526.3110 - mae: 41.1024 - val_loss: 3641.9150 - val_mae: 44.9649\n",
            "Epoch 16/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 2278.0628 - mae: 38.1770 - val_loss: 3913.9019 - val_mae: 47.3276\n",
            "Epoch 17/2000\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 2333.1188 - mae: 38.7246 - val_loss: 4052.0947 - val_mae: 46.9321\n",
            "Epoch 18/2000\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 2923.9197 - mae: 43.3780 - val_loss: 3737.6619 - val_mae: 46.3438\n",
            "Epoch 19/2000\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 2588.2695 - mae: 42.4527 - val_loss: 3622.1824 - val_mae: 45.4205\n",
            "Epoch 20/2000\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 2202.4833 - mae: 37.9402 - val_loss: 3673.8823 - val_mae: 46.1684\n",
            "Epoch 21/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 2242.5116 - mae: 37.9239 - val_loss: 4152.2485 - val_mae: 49.8782\n",
            "Epoch 22/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 2729.1809 - mae: 42.6276 - val_loss: 3793.6931 - val_mae: 46.6623\n",
            "Epoch 23/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 2406.3517 - mae: 39.1140 - val_loss: 4182.4868 - val_mae: 47.7078\n",
            "Epoch 24/2000\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 2336.3420 - mae: 39.0740 - val_loss: 3898.6550 - val_mae: 46.6270\n",
            "Epoch 25/2000\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 2205.2400 - mae: 36.5687 - val_loss: 3796.1755 - val_mae: 46.5861\n",
            "Epoch 26/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 2403.8581 - mae: 39.9834 - val_loss: 3760.0833 - val_mae: 46.6446\n",
            "Epoch 27/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 2104.1917 - mae: 37.2357 - val_loss: 3793.6677 - val_mae: 47.3589\n",
            "Epoch 28/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 2463.0845 - mae: 40.7848 - val_loss: 4010.4473 - val_mae: 46.5589\n",
            "Epoch 29/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 2294.8450 - mae: 37.8999 - val_loss: 3914.7883 - val_mae: 47.2232\n",
            "Epoch 30/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 2097.5366 - mae: 36.1060 - val_loss: 3924.7981 - val_mae: 45.8372\n",
            "Epoch 31/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 1975.7036 - mae: 35.7224 - val_loss: 3814.0122 - val_mae: 46.4933\n",
            "Epoch 32/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 2161.6737 - mae: 37.6476 - val_loss: 4151.3203 - val_mae: 48.0457\n",
            "Epoch 33/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 2412.7340 - mae: 39.4036 - val_loss: 3992.3513 - val_mae: 46.5981\n",
            "Epoch 34/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 2096.5592 - mae: 36.6610 - val_loss: 4130.0850 - val_mae: 49.4272\n",
            "Epoch 35/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 1927.9023 - mae: 35.2058 - val_loss: 3968.3157 - val_mae: 48.2891\n",
            "Epoch 36/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 1712.3095 - mae: 32.3491 - val_loss: 4404.4707 - val_mae: 49.3005\n",
            "Epoch 37/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 2571.6648 - mae: 41.2487 - val_loss: 4029.7600 - val_mae: 46.9827\n",
            "Epoch 38/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 2330.5286 - mae: 38.9295 - val_loss: 4071.0703 - val_mae: 48.6015\n",
            "Epoch 39/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 2033.7765 - mae: 35.8225 - val_loss: 4090.3560 - val_mae: 50.3423\n",
            "Epoch 40/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 2328.4247 - mae: 39.6029 - val_loss: 4143.9102 - val_mae: 48.1092\n",
            "Epoch 41/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 2238.9649 - mae: 37.7634 - val_loss: 4194.7661 - val_mae: 51.0164\n",
            "Epoch 42/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 1980.2132 - mae: 35.9762 - val_loss: 4263.8994 - val_mae: 48.3164\n",
            "Epoch 43/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 1951.8439 - mae: 35.6239 - val_loss: 4024.6892 - val_mae: 48.5904\n",
            "Epoch 44/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 1952.6618 - mae: 35.3272 - val_loss: 4383.4824 - val_mae: 48.4870\n",
            "Epoch 45/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 1882.2022 - mae: 34.0120 - val_loss: 4325.1626 - val_mae: 48.1566\n",
            "Epoch 46/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 2313.1943 - mae: 39.0545 - val_loss: 4397.8330 - val_mae: 49.1167\n",
            "Epoch 47/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 1757.2258 - mae: 33.7885 - val_loss: 4345.1455 - val_mae: 51.8623\n",
            "Epoch 48/2000\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 1901.5009 - mae: 35.0552 - val_loss: 4257.4932 - val_mae: 46.7394\n",
            "Epoch 49/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 1961.6876 - mae: 36.7287 - val_loss: 4362.4575 - val_mae: 50.7289\n",
            "Epoch 50/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 1942.4342 - mae: 36.7401 - val_loss: 4161.7925 - val_mae: 47.2643\n",
            "Epoch 51/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 1656.5425 - mae: 33.2573 - val_loss: 4363.9121 - val_mae: 49.3510\n",
            "Epoch 52/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 1511.8731 - mae: 30.4514 - val_loss: 4094.1082 - val_mae: 48.6792\n",
            "Epoch 53/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 1515.6124 - mae: 30.5921 - val_loss: 4272.6392 - val_mae: 48.0405\n",
            "Epoch 54/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 1622.8263 - mae: 31.5605 - val_loss: 4157.7256 - val_mae: 48.9553\n",
            "Epoch 55/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 1771.6054 - mae: 34.2329 - val_loss: 4385.6958 - val_mae: 49.1962\n",
            "Epoch 56/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 1411.5031 - mae: 29.8697 - val_loss: 4853.6626 - val_mae: 56.8443\n",
            "Epoch 57/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 2183.4917 - mae: 36.4918 - val_loss: 4068.6531 - val_mae: 46.7832\n",
            "Epoch 58/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 1931.9430 - mae: 34.7855 - val_loss: 4380.6753 - val_mae: 47.7498\n",
            "Epoch 59/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 1919.8884 - mae: 35.8252 - val_loss: 4120.9380 - val_mae: 47.6453\n",
            "Epoch 60/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 1426.6620 - mae: 30.6848 - val_loss: 4785.3628 - val_mae: 51.7464\n",
            "Epoch 61/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 1802.3719 - mae: 34.9143 - val_loss: 4243.7676 - val_mae: 50.1811\n",
            "Epoch 62/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 1559.0587 - mae: 31.4864 - val_loss: 4355.2495 - val_mae: 48.5897\n",
            "Epoch 63/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 1571.2309 - mae: 30.6048 - val_loss: 4600.3765 - val_mae: 50.2935\n",
            "Epoch 64/2000\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 1927.1080 - mae: 34.9362 - val_loss: 4506.2886 - val_mae: 51.1947\n",
            "Epoch 65/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 1383.6233 - mae: 29.7325 - val_loss: 4460.5249 - val_mae: 47.6700\n",
            "Epoch 66/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 1516.1083 - mae: 31.9264 - val_loss: 4589.2104 - val_mae: 50.9976\n",
            "Epoch 67/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 1490.5658 - mae: 31.7038 - val_loss: 4419.0210 - val_mae: 49.0628\n",
            "Epoch 68/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 1362.3406 - mae: 28.6508 - val_loss: 4606.9058 - val_mae: 49.0649\n",
            "Epoch 69/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 1382.6291 - mae: 29.7967 - val_loss: 4646.2739 - val_mae: 49.9457\n",
            "Epoch 70/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 1436.7954 - mae: 30.7214 - val_loss: 4800.7822 - val_mae: 50.5071\n",
            "Epoch 71/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 1408.3580 - mae: 29.8190 - val_loss: 4488.5220 - val_mae: 52.4477\n",
            "Epoch 72/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 1459.8023 - mae: 31.3969 - val_loss: 4503.4600 - val_mae: 48.8097\n",
            "Epoch 73/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 1464.3754 - mae: 30.1333 - val_loss: 4348.0635 - val_mae: 50.1448\n",
            "Epoch 74/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 1408.5599 - mae: 29.2887 - val_loss: 5083.5264 - val_mae: 56.2140\n",
            "Epoch 75/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 1289.7952 - mae: 29.5760 - val_loss: 4605.4878 - val_mae: 49.3741\n",
            "Epoch 76/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 1370.2528 - mae: 29.6283 - val_loss: 4940.6309 - val_mae: 55.4122\n",
            "Epoch 77/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 1166.3801 - mae: 27.5724 - val_loss: 5055.9912 - val_mae: 52.4110\n",
            "Epoch 78/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 1066.6484 - mae: 25.8643 - val_loss: 4714.7129 - val_mae: 51.2781\n",
            "Epoch 79/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 1069.8647 - mae: 25.7679 - val_loss: 5480.7129 - val_mae: 55.5255\n",
            "Epoch 80/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 1362.7966 - mae: 28.3312 - val_loss: 5214.9780 - val_mae: 55.9505\n",
            "Epoch 81/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 1320.2057 - mae: 29.1440 - val_loss: 4676.5493 - val_mae: 51.6054\n",
            "Epoch 82/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 1198.1964 - mae: 27.0262 - val_loss: 4480.5073 - val_mae: 48.6209\n",
            "Epoch 83/2000\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 1075.9356 - mae: 26.6001 - val_loss: 5118.0244 - val_mae: 52.2930\n",
            "Epoch 84/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 1265.0638 - mae: 29.6667 - val_loss: 4730.2461 - val_mae: 52.0794\n",
            "Epoch 85/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 1211.1337 - mae: 27.6352 - val_loss: 4978.5425 - val_mae: 50.8916\n",
            "Epoch 86/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 1515.7702 - mae: 31.5372 - val_loss: 4611.1538 - val_mae: 51.6803\n",
            "Epoch 87/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 1073.4889 - mae: 26.0662 - val_loss: 4468.7271 - val_mae: 48.6744\n",
            "Epoch 88/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 1148.3941 - mae: 27.9438 - val_loss: 4663.4878 - val_mae: 53.7524\n",
            "Epoch 89/2000\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 915.5565 - mae: 24.2415 - val_loss: 5124.0913 - val_mae: 54.5097\n",
            "Epoch 90/2000\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 1201.3476 - mae: 27.9425 - val_loss: 5048.3818 - val_mae: 54.4462\n",
            "Epoch 91/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 1232.0896 - mae: 27.5301 - val_loss: 4563.6997 - val_mae: 51.3006\n",
            "Epoch 92/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 906.4013 - mae: 24.1995 - val_loss: 4988.8877 - val_mae: 52.7890\n",
            "Epoch 93/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 885.1908 - mae: 23.3573 - val_loss: 4917.6372 - val_mae: 55.1391\n",
            "Epoch 94/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 1099.0569 - mae: 26.0969 - val_loss: 5515.4707 - val_mae: 55.5759\n",
            "Epoch 95/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 1094.3600 - mae: 26.8920 - val_loss: 5340.8701 - val_mae: 56.7358\n",
            "Epoch 96/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 1026.7200 - mae: 26.0196 - val_loss: 4948.1533 - val_mae: 52.5512\n",
            "Epoch 97/2000\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 834.9386 - mae: 22.6657 - val_loss: 5098.4209 - val_mae: 53.1341\n",
            "Epoch 98/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 974.7858 - mae: 25.5538 - val_loss: 5278.5361 - val_mae: 53.3658\n",
            "Epoch 99/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 1039.7500 - mae: 26.1657 - val_loss: 5499.9551 - val_mae: 56.9788\n",
            "Epoch 100/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 818.8410 - mae: 22.5537 - val_loss: 5321.1763 - val_mae: 55.2957\n",
            "Epoch 101/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 1013.4574 - mae: 25.6250 - val_loss: 4686.7578 - val_mae: 51.1651\n",
            "Epoch 102/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 797.4914 - mae: 22.1161 - val_loss: 5356.6270 - val_mae: 56.0627\n",
            "Epoch 103/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 949.0750 - mae: 23.9225 - val_loss: 4856.7695 - val_mae: 51.8344\n",
            "Epoch 104/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 531.7009 - mae: 18.4106 - val_loss: 5162.9443 - val_mae: 52.5502\n",
            "Epoch 105/2000\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 818.7623 - mae: 22.1484 - val_loss: 5432.9858 - val_mae: 56.7188\n",
            "Epoch 106/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 662.4155 - mae: 19.9938 - val_loss: 5464.5649 - val_mae: 54.6263\n",
            "Epoch 107/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 987.8604 - mae: 25.0214 - val_loss: 4941.5981 - val_mae: 52.7609\n",
            "Epoch 108/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 613.8542 - mae: 19.9333 - val_loss: 5264.8091 - val_mae: 54.8404\n",
            "Epoch 109/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 597.5975 - mae: 19.7223 - val_loss: 5652.1738 - val_mae: 56.6365\n",
            "Epoch 110/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 945.3482 - mae: 23.8712 - val_loss: 5169.1621 - val_mae: 54.2112\n",
            "Epoch 111/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 1015.3402 - mae: 25.4583 - val_loss: 5260.6313 - val_mae: 53.9967\n",
            "Epoch 112/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 657.7665 - mae: 20.7246 - val_loss: 5377.0801 - val_mae: 55.6977\n",
            "Epoch 113/2000\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 656.9324 - mae: 20.1185 - val_loss: 5338.3638 - val_mae: 55.2076\n",
            "Epoch 114/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 584.4798 - mae: 19.4494 - val_loss: 5563.0723 - val_mae: 56.0733\n",
            "Epoch 115/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 688.0571 - mae: 21.6465 - val_loss: 5724.8569 - val_mae: 59.6596\n",
            "Epoch 116/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 591.8599 - mae: 18.9850 - val_loss: 5253.6323 - val_mae: 55.0793\n",
            "Epoch 117/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 960.9362 - mae: 23.7784 - val_loss: 5388.0591 - val_mae: 55.0630\n",
            "Epoch 118/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 601.7771 - mae: 19.4066 - val_loss: 5336.5923 - val_mae: 55.6684\n",
            "Epoch 119/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 710.0620 - mae: 21.5636 - val_loss: 5252.4277 - val_mae: 56.9456\n",
            "Epoch 120/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 617.3586 - mae: 19.1788 - val_loss: 5574.9268 - val_mae: 58.5677\n",
            "Epoch 121/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 560.9668 - mae: 18.6409 - val_loss: 5414.2656 - val_mae: 56.5944\n",
            "Epoch 122/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 488.2714 - mae: 17.4229 - val_loss: 5967.4946 - val_mae: 60.0611\n",
            "Epoch 123/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 880.2509 - mae: 23.8054 - val_loss: 5404.9434 - val_mae: 56.0706\n",
            "Epoch 124/2000\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 568.1364 - mae: 18.6682 - val_loss: 5359.9077 - val_mae: 57.0930\n",
            "Epoch 125/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 499.1436 - mae: 18.0139 - val_loss: 5826.0552 - val_mae: 57.5706\n",
            "Epoch 126/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 469.1953 - mae: 17.3071 - val_loss: 5557.9707 - val_mae: 57.3718\n",
            "Epoch 127/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 550.9735 - mae: 18.7900 - val_loss: 5304.0249 - val_mae: 56.5714\n",
            "Epoch 128/2000\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 548.7101 - mae: 19.1728 - val_loss: 5689.0850 - val_mae: 58.2383\n",
            "Epoch 129/2000\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 547.7177 - mae: 18.3476 - val_loss: 4908.0386 - val_mae: 54.0155\n",
            "Epoch 130/2000\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 754.9235 - mae: 22.3040 - val_loss: 5812.5967 - val_mae: 58.7398\n",
            "Epoch 131/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 459.8413 - mae: 16.4284 - val_loss: 5349.4712 - val_mae: 55.6525\n",
            "Epoch 132/2000\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 379.1331 - mae: 14.6491 - val_loss: 5628.8096 - val_mae: 58.5820\n",
            "Epoch 133/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 526.2079 - mae: 18.1014 - val_loss: 5657.0596 - val_mae: 58.2529\n",
            "Epoch 134/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 536.5488 - mae: 18.7327 - val_loss: 5576.6899 - val_mae: 56.8201\n",
            "Epoch 135/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 477.4642 - mae: 17.6870 - val_loss: 5672.3013 - val_mae: 57.6737\n",
            "Epoch 136/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 466.6644 - mae: 16.8259 - val_loss: 5634.4995 - val_mae: 56.8902\n",
            "Epoch 137/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 529.1159 - mae: 18.8764 - val_loss: 5482.2612 - val_mae: 57.2338\n",
            "Epoch 138/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 442.4843 - mae: 16.8958 - val_loss: 5766.0742 - val_mae: 58.1607\n",
            "Epoch 139/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 399.0360 - mae: 15.7296 - val_loss: 6540.4233 - val_mae: 60.2062\n",
            "Epoch 140/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 594.1925 - mae: 19.2379 - val_loss: 6192.8232 - val_mae: 60.4001\n",
            "Epoch 141/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 705.7544 - mae: 21.3176 - val_loss: 5033.7925 - val_mae: 53.7580\n",
            "Epoch 142/2000\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 467.5152 - mae: 17.6160 - val_loss: 5738.5845 - val_mae: 59.2157\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 5716.9614 - mae: 56.7841\n",
            "loss :  5716.96142578125\n",
            "mae :  56.784114837646484\n",
            "r2 :  -0.0314217866413411\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCQVr1tZOxaQ"
      },
      "source": [
        "M"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}